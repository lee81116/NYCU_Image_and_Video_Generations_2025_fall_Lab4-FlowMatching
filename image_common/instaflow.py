from typing import Optional

import torch
import torch.nn as nn

try:
    import lpips
    LPIPS_AVAILABLE = True
except ImportError:
    LPIPS_AVAILABLE = False
    print("Warning: lpips not available. Install with: pip install lpips")


class InstaFlowModel(nn.Module):
    """
    This class represents the distilled, one-step "student" model.
    It is trained to emulate the *final output* of the guided "teacher" model.

    Can optionally use LPIPS perceptual loss for better visual quality.
    """
    def __init__(self, network: nn.Module, use_lpips: bool = False, **kwargs):
        super().__init__()
        self.network = network
        self.use_lpips = use_lpips and LPIPS_AVAILABLE

        if self.use_lpips:
            # Initialize LPIPS network (AlexNet-based perceptual loss)
            self.lpips_fn = lpips.LPIPS(net='alex')
            # Freeze LPIPS network parameters
            for param in self.lpips_fn.parameters():
                param.requires_grad = False
            print("InstaFlow: Using LPIPS perceptual loss (L2 + LPIPS)")

    @property
    def device(self):
        return next(self.network.parameters()).device

    @property
    def image_resolution(self):
        return self.network.image_resolution

    def get_loss(self, x1, x0, class_label=None):
        """
        The objective for InstaFlow one-step generator, refer to Eq. 6 in 
        Liu et al., "InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation".

        Given noise data x0, the model should generate high-quality image x1 in one step.
        """
        assert x0 is not None, "x0 (noise input) must be provided for InstaFlow training."

        ######## TODO ########
        # DO NOT change the code outside this part.
        # Implement the InstaFlow distillation objective (Eq. 6)
        #
        # Given:
        #   x1: the target image (pre-generated by the multi-step "teacher" with CFG)
        #   x0: the initial noise
        #   class_label: the class label (T)
        #
        # Steps to complete:
        # 1. Create a tensor of zeros for t=0 with the same batch size and device as x0
        #    Hint: torch.zeros(x0.shape[0], device=self.device)
        #
        # 2. Get the student's predicted velocity v(x0, 0 | T)
        #    Call self.network(x0, t_zero, class_label=class_label) if class_label is provided
        #
        # 3. Calculate the student's final predicted image
        #    Formula: x1_pred = x0 + v_pred
        #
        # 4. Calculate the L2 loss (MSE between x1_pred and x1)
        #    Hint: use .pow(2).mean()
        #
        # 5. If using LPIPS, add perceptual loss
        #    LPIPS expects inputs in [-1, 1] range

        t_zero = None  # TODO: create t=0 tensor
        v_pred = None  # TODO: get predicted velocity
        x1_pred = None  # TODO: compute predicted image
        l2_loss = None  # TODO: compute MSE loss

        # Combine L2 + LPIPS loss
        if self.use_lpips:
            # LPIPS expects inputs in [-1, 1], clamp to be safe
            x1_clamped = torch.clamp(x1, -1, 1)
            x1_pred_clamped = torch.clamp(x1_pred, -1, 1)
            lpips_loss = self.lpips_fn(x1_pred_clamped, x1_clamped).mean()
            loss = l2_loss + lpips_loss
        else:
            loss = l2_loss
        ######################

        return loss

    @torch.no_grad()
    def sample(
        self,
        shape,
        class_label: Optional[torch.Tensor] = None,
    ):
        """
        Inference for the one-step "student" model.
        """
        batch_size = shape[0]
        # Get the initial noise (x_0)
        x_T = torch.randn(shape).to(self.device)

        ######## TODO ########
        # Complete the one-step sampling
        #
        # Steps to complete:
        # 1. Set x_0 = x_T (the initial noise)
        #
        # 2. Create t=0 tensor: torch.zeros(x_0.shape[0], device=self.device)
        #
        # 3. Get the predicted velocity from the student model
        #    Call self.network(x_0, t_zero, class_label=class_label) if class_label is provided
        #    Note: We do NOT apply CFG math here - the CFG effect is "baked in" during training
        #
        # 4. Calculate the final image: x_1 = x_0 + v_pred
        
        x_0 = None  # TODO: set initial noise
        t_zero = None  # TODO: create t=0 tensor
        v_pred = None  # TODO: get predicted velocity
        x_1 = None  # TODO: compute final image
        ######################

        return x_1

    def save(self, file_path):
        hparams = {
            "network": self.network,
        }
        state_dict = self.state_dict()

        dic = {"hparams": hparams, "state_dict": state_dict}
        torch.save(dic, file_path)

    def load(self, file_path):
        dic = torch.load(file_path, map_location="cpu")
        hparams = dic["hparams"]
        state_dict = dic["state_dict"]

        self.network = hparams["network"]

        self.load_state_dict(state_dict)